{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"finalData.tsv\", 'r') as fp:\n",
    "\tdata = fp.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "\tdata[i] = data[i].strip('\\n')\n",
    "\tdata[i] = data[i].split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "currPoint = []\n",
    "\n",
    "for token in data:\n",
    "\tif token[0] == '':\n",
    "\t\tif len(currPoint) > 0:\n",
    "\t\t\ttweets.append(currPoint)\n",
    "\t\t\tcurrPoint = []\n",
    "\telse:\n",
    "\t\tcurrPoint.append(token)\n",
    "\n",
    "print len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = np.array(tweets)\n",
    "np.random.seed(52)\n",
    "np.random.shuffle(tweets)\n",
    "tweets = tweets.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsVal = tweets[int(len(tweets) * 0.8):]\n",
    "tweets = tweets[:int(len(tweets) * 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298, 1191)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweetsVal), len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Baseline performance (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Creating map ...\n",
      "--> Testing ...\n",
      " CRF Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ     0.8210    0.6205    0.7068       303\n",
      "        ADP     0.9435    0.9153    0.9292       602\n",
      "        ADV     0.9085    0.7028    0.7926       212\n",
      "       CONJ     0.9603    0.9119    0.9355       159\n",
      "        DET     0.9321    0.9279    0.9300       222\n",
      "       NOUN     0.9221    0.5956    0.7237       994\n",
      "        NUM     0.9841    0.7848    0.8732        79\n",
      "       PART     0.8052    0.7570    0.7804       284\n",
      "   PART_NEG     0.9659    1.0000    0.9827        85\n",
      "       PRON     0.9542    0.8092    0.8757       283\n",
      "    PRON_WH     0.9595    0.8987    0.9281        79\n",
      "      PROPN     0.9515    0.7140    0.8158       549\n",
      "       VERB     0.9518    0.7353    0.8297      1156\n",
      "          X     0.6163    0.9950    0.7611      1593\n",
      "\n",
      "avg / total     0.8522    0.8061    0.8077      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"--> Creating map ...\"\n",
    "\n",
    "tagDict = {}\n",
    "\n",
    "allTags = []\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        allTags.append(token[2])\n",
    "tags, counts = np.unique(allTags, return_counts = True)\n",
    "mostFrequentTag = tags[np.argmax(counts)]\n",
    "\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        if token[0] in tagDict:\n",
    "            pass\n",
    "        else:\n",
    "            tagDict[token[0]] = []\n",
    "        \n",
    "        tagDict[token[0]].append(token[2])\n",
    "        \n",
    "print \"--> Testing ...\"\n",
    "\n",
    "predictedLabels = []\n",
    "correctLabels = []\n",
    "count = 0\n",
    "\n",
    "for tweet in tweetsVal:\n",
    "    for token in tweet:\n",
    "        correctLabels.append(token[2])\n",
    "        \n",
    "        if token[0] in tagDict:\n",
    "            tags, counts = np.unique(tagDict[token[0]], return_counts = True)\n",
    "            predictedLabels.append(tags[np.argmax(counts)])\n",
    "        else:\n",
    "            count += 1\n",
    "            predictedLabels.append(mostFrequentTag)\n",
    "\n",
    "print \"\"\" CRF Classification\"\"\"\n",
    "print classification_report(y_true = correctLabels, y_pred = predictedLabels, digits = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline performance (token + language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Creating map ...\n",
      "--> Testing ...\n",
      " CRF Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ADJ     0.8186    0.6106    0.6994       303\n",
      "        ADP     0.9635    0.9203    0.9414       602\n",
      "        ADV     0.9255    0.7028    0.7989       212\n",
      "       CONJ     0.9730    0.9057    0.9381       159\n",
      "        DET     0.9631    0.9414    0.9522       222\n",
      "       NOUN     0.7198    0.8038    0.7595       994\n",
      "        NUM     0.9841    0.7848    0.8732        79\n",
      "       PART     0.8139    0.7852    0.7993       284\n",
      "   PART_NEG     0.9659    1.0000    0.9827        85\n",
      "       PRON     0.9796    0.8481    0.9091       283\n",
      "    PRON_WH     0.9600    0.9114    0.9351        79\n",
      "      PROPN     0.9676    0.7067    0.8168       549\n",
      "       VERB     0.7508    0.8443    0.7948      1156\n",
      "          X     0.9144    0.9856    0.9486      1593\n",
      "\n",
      "avg / total     0.8649    0.8570    0.8564      6600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"--> Creating map ...\"\n",
    "\n",
    "tagDict = {}\n",
    "\n",
    "allTags = []\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        if token[1] == 'en':\n",
    "            allTags.append(token[2])\n",
    "tags, counts = np.unique(allTags, return_counts = True)\n",
    "mostFrequentTagEn = tags[np.argmax(counts)]\n",
    "\n",
    "allTags = []\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        if token[1] == 'hi':\n",
    "            allTags.append(token[2])\n",
    "tags, counts = np.unique(allTags, return_counts = True)\n",
    "mostFrequentTagHi = tags[np.argmax(counts)]\n",
    "\n",
    "allTags = []\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        if token[1] == 'rest':\n",
    "            allTags.append(token[2])\n",
    "tags, counts = np.unique(allTags, return_counts = True)\n",
    "mostFrequentTagRest = tags[np.argmax(counts)]\n",
    "\n",
    "for tweet in tweets:\n",
    "    for token in tweet:\n",
    "        if token[0] + '__' + token[1] in tagDict:\n",
    "            pass\n",
    "        else:\n",
    "            tagDict[token[0] + '__' + token[1]] = []\n",
    "        \n",
    "        tagDict[token[0] + '__' + token[1]].append(token[2])\n",
    "        \n",
    "print \"--> Testing ...\"\n",
    "\n",
    "predictedLabels = []\n",
    "correctLabels = []\n",
    "count = 0\n",
    "\n",
    "for tweet in tweetsVal:\n",
    "    for token in tweet:\n",
    "        correctLabels.append(token[2])\n",
    "        \n",
    "        if token[0] + '__' + token[1] in tagDict:\n",
    "            tags, counts = np.unique(tagDict[token[0] + '__' + token[1]], return_counts = True)\n",
    "            predictedLabels.append(tags[np.argmax(counts)])\n",
    "        else:\n",
    "            if token[1] == 'en':\n",
    "                predictedLabels.append(mostFrequentTagEn)\n",
    "            elif token[1] == 'hi':\n",
    "                predictedLabels.append(mostFrequentTagHi)\n",
    "            if token[1] == 'rest':\n",
    "                predictedLabels.append(mostFrequentTagRest)\n",
    "\n",
    "print \"\"\" CRF Classification\"\"\"\n",
    "print classification_report(y_true = correctLabels, y_pred = predictedLabels, digits = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
